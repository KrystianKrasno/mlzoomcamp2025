{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d1a40f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lead_source</th>\n",
       "      <td>paid_ads</td>\n",
       "      <td>social_media</td>\n",
       "      <td>events</td>\n",
       "      <td>paid_ads</td>\n",
       "      <td>referral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>industry</th>\n",
       "      <td>NaN</td>\n",
       "      <td>retail</td>\n",
       "      <td>healthcare</td>\n",
       "      <td>retail</td>\n",
       "      <td>education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_courses_viewed</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annual_income</th>\n",
       "      <td>79450.0</td>\n",
       "      <td>46992.0</td>\n",
       "      <td>78796.0</td>\n",
       "      <td>83843.0</td>\n",
       "      <td>85012.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>employment_status</th>\n",
       "      <td>unemployed</td>\n",
       "      <td>employed</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>self_employed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <td>south_america</td>\n",
       "      <td>south_america</td>\n",
       "      <td>australia</td>\n",
       "      <td>australia</td>\n",
       "      <td>europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>interaction_count</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lead_score</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>converted</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      0              1           2          3  \\\n",
       "lead_source                    paid_ads   social_media      events   paid_ads   \n",
       "industry                            NaN         retail  healthcare     retail   \n",
       "number_of_courses_viewed              1              1           5          2   \n",
       "annual_income                   79450.0        46992.0     78796.0    83843.0   \n",
       "employment_status            unemployed       employed  unemployed        NaN   \n",
       "location                  south_america  south_america   australia  australia   \n",
       "interaction_count                     4              1           3          1   \n",
       "lead_score                         0.94            0.8        0.69       0.87   \n",
       "converted                             1              0           1          0   \n",
       "\n",
       "                                      4  \n",
       "lead_source                    referral  \n",
       "industry                      education  \n",
       "number_of_courses_viewed              3  \n",
       "annual_income                   85012.0  \n",
       "employment_status         self_employed  \n",
       "location                         europe  \n",
       "interaction_count                     3  \n",
       "lead_score                         0.62  \n",
       "converted                             1  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# In this dataset our desired target for classification task will be converted variable - has the client signed up to the platform or not.\n",
    "\n",
    "df = pd.read_csv(\"course_lead_scoring.csv\")\n",
    "\n",
    "# print(df.head(5))\n",
    "# print(df.dtypes)\n",
    "\n",
    "df.head().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "51f5b4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1462 entries, 0 to 1461\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   lead_source               1334 non-null   object \n",
      " 1   industry                  1328 non-null   object \n",
      " 2   number_of_courses_viewed  1462 non-null   int64  \n",
      " 3   annual_income             1281 non-null   float64\n",
      " 4   employment_status         1362 non-null   object \n",
      " 5   location                  1399 non-null   object \n",
      " 6   interaction_count         1462 non-null   int64  \n",
      " 7   lead_score                1462 non-null   float64\n",
      " 8   converted                 1462 non-null   int64  \n",
      "dtypes: float64(2), int64(3), object(4)\n",
      "memory usage: 102.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "    # Check if the missing values are presented in the features.\n",
    "df.isnull().sum()\n",
    "df.info() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "58c35506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lead_source', 'industry', 'employment_status', 'location']\n",
      "['number_of_courses_viewed', 'annual_income', 'interaction_count', 'lead_score', 'converted']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lead_source                 0\n",
       "industry                    0\n",
       "number_of_courses_viewed    0\n",
       "annual_income               0\n",
       "employment_status           0\n",
       "location                    0\n",
       "interaction_count           0\n",
       "lead_score                  0\n",
       "converted                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If there are missing values:\n",
    "    # For categorical features, replace them with 'NA'\n",
    "    # For numerical features, replace with with 0.0\n",
    "\n",
    "cat_features = list(df.dtypes[df.dtypes == 'object'].index)\n",
    "print (cat_features)\n",
    "\n",
    "num_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "print(num_features)\n",
    "\n",
    "for col in cat_features: \n",
    "    df[col] = df[col].fillna('NA')\n",
    "\n",
    "for col in num_features:\n",
    "    df[col] = df[col].fillna(0)\n",
    "\n",
    "\n",
    "df.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "36e803d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode for 'industry': retail\n",
      "{'interaction_count and lead_score': np.float64(0.009888182496913131), 'number_of_courses_viewed and lead_score': np.float64(-0.004878998354681276), 'number_of_courses_viewed and interaction_count': np.float64(-0.023565222882888037), 'annual_income and interaction_count': np.float64(0.02703647240481443)}\n",
      "Pair with the biggest correlation: annual_income and interaction_count\n"
     ]
    }
   ],
   "source": [
    "# Question 1\n",
    "    # What is the most frequent observation (mode) for the column industry?\n",
    "\n",
    "most_freq = df['industry'].mode()[0]\n",
    "print(f\"Mode for 'industry': {most_freq}\")\n",
    "\n",
    "# Question 2\n",
    "# Create the correlation matrix for the numerical features of your dataset. In a correlation matrix, you compute the correlation coefficient between every pair of features.\n",
    "corr_matrix = df[num_features].corr()\n",
    "\n",
    "# Check the correlation values for the specified pairs (absolute value indicates strength)\n",
    "corr_interaction_leadscore = corr_matrix.loc['interaction_count', 'lead_score']\n",
    "corr_courses_leadscore = corr_matrix.loc['number_of_courses_viewed', 'lead_score']\n",
    "corr_courses_interaction = corr_matrix.loc['number_of_courses_viewed', 'interaction_count']\n",
    "corr_annualincome_interaction = corr_matrix.loc['annual_income', 'interaction_count']\n",
    "\n",
    "# Create a dictionary to hold the absolute correlations\n",
    "abs_correlations = {\n",
    "    'interaction_count and lead_score': (corr_interaction_leadscore),\n",
    "    'number_of_courses_viewed and lead_score': (corr_courses_leadscore),\n",
    "    'number_of_courses_viewed and interaction_count': (corr_courses_interaction),\n",
    "    'annual_income and interaction_count': (corr_annualincome_interaction)\n",
    "}\n",
    "\n",
    "print(abs_correlations)\n",
    "\n",
    "# # Find the pair with the maximum absolute correlation\n",
    "biggest_correlation_pair = max(abs_correlations, key=abs_correlations.get)\n",
    "print(f\"Pair with the biggest correlation: {biggest_correlation_pair}\")\n",
    "\n",
    "# What are the two features that have the biggest correlation?\n",
    "    # interaction_count and lead_score\n",
    "    # number_of_courses_viewed and lead_score\n",
    "    # number_of_courses_viewed and interaction_count\n",
    "                # annual_income and interaction_count  *******answer \n",
    "    # Only consider the pairs above when answering this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fcc275ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1462\n",
      "1169\n",
      "876\n",
      "293\n",
      "293\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    " # Split the data\n",
    "    # Split your data in train/val/test sets with 60%/20%/20% distribution.\n",
    "    # Use Scikit-Learn for that (the train_test_split function) and set the seed to 42.\n",
    "    # Make sure that the target value converted is not in your dataframe.\n",
    "\n",
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\n",
    "\n",
    "df_train, df_val = train_test_split(df_full_train, test_size = 0.25, random_state=42)\n",
    "# split the df_train which was 80% of the original df as 0.25 * 0.80 = .20) \n",
    "\n",
    "print(len(df))\n",
    "print(len(df_full_train))\n",
    "print(len(df_train))\n",
    "print(len(df_test))   \n",
    "print(len(df_val))\n",
    "                                    \n",
    "# Separate target variables\n",
    "y_train = df_train[\"converted\"].values\n",
    "y_val = df_val[\"converted\"].values\n",
    "y_test = df_test[\"converted\"].values\n",
    "\n",
    "# Drop the target variable from feature dataframes\n",
    "df_train = df_train.drop(columns=[\"converted\"])\n",
    "df_val = df_val.drop(columns=[\"converted\"])\n",
    "df_test = df_test.drop(columns=[\"converted\"])         \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "542355de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "industry MI =  0.01\n",
      "location MI =  0.0\n",
      "lead_source MI =  0.03\n",
      "employment_status MI =  0.01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lead_source          0.02\n",
       "industry             0.01\n",
       "employment_status    0.01\n",
       "location             0.00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 3\n",
    "    # Calculate the mutual information score between converted and other categorical variables in the dataset. Use the training set only.\n",
    "    # Round the scores to 2 decimals using round(score, 2).\n",
    "# Which of these variables has the biggest mutual information score?\n",
    "    # industry\n",
    "    # location\n",
    "          # lead_source  ** largest at 0.04 \n",
    "    # employment_status\n",
    "\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "mi_industry = mutual_info_score(y_train, df_train.industry)\n",
    "mi_location = mutual_info_score(y_train, df_train.location)\n",
    "mi_lead_source = mutual_info_score(y_train, df_train.lead_source)\n",
    "mi_employment_status = mutual_info_score(y_train, df_train.employment_status)\n",
    "\n",
    "# The first time I did this I just used df_full_train on everything above and got a very similar answer (0.03 - lead_source) .. would this have been incorrect? \n",
    "\n",
    "print(\"industry MI = \", round(mi_industry,2))\n",
    "print(\"location MI = \", round(mi_location,2))\n",
    "print(\"lead_source MI = \", round(mi_lead_source,2))\n",
    "print(\"employment_status MI = \", round(mi_employment_status,2))\n",
    "\n",
    "\n",
    "# or ...\n",
    "\n",
    "\n",
    "def mutual_info_churn_score(series):\n",
    "    return mutual_info_score(series, df_full_train.converted)\n",
    "\n",
    "mi = df_full_train[cat_features].apply(mutual_info_churn_score).round(2)\n",
    "mi.sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d47595ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7304\n"
     ]
    }
   ],
   "source": [
    "# Question 4\n",
    "# Now let's train a logistic regression.\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Check and remove 'converted' from numerical features list\n",
    "if 'converted' in num_features:\n",
    "    num_features.remove('converted')\n",
    "    \n",
    "# Check and remove 'converted' from categorical features list if it was mistakenly there\n",
    "if 'converted' in cat_features:\n",
    "    cat_features.remove('converted')\n",
    "\n",
    "# Remember that we have several categorical variables in the dataset. Include them using one-hot encoding.\n",
    "all_features = cat_features +  num_features\n",
    "\n",
    "train_dicts = df_train[all_features].to_dict(orient='records') # For making dict row-wise we use orient = 'records\n",
    "val_dicts = df_val[all_features].to_dict(orient='records')\n",
    "\n",
    "dv = DictVectorizer(sparse=False)\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "X_val = dv.transform(val_dicts)\n",
    "\n",
    "# Fit the model on the training dataset.\n",
    "# To make sure the results are reproducible across different versions of Scikit-Learn, fit the model with these parameters:\n",
    "# model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "\n",
    "model = LogisticRegression(solver = 'liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "accuracy = (y_val == y_pred).mean()\n",
    "\n",
    "acc_round = round(accuracy, 4)\n",
    "\n",
    "\n",
    "# Calculate the accuracy on the validation dataset and round it to 2 decimal digits.\n",
    "# What accuracy did you get?\n",
    "    # 0.64\n",
    "     # 0.74  ** this one is the closest but I get 0.7304 \n",
    "    # 0.84\n",
    "    # 0.94\n",
    "\n",
    "print(acc_round)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "22efada1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.   , -0.024,  0.044, -0.01 ,  0.001, -0.096, -0.028,  0.038,\n",
       "       -0.005, -0.034,  0.001, -0.018, -0.033, -0.006,  0.297,  0.048,\n",
       "        0.008, -0.012, -0.012, -0.111,  0.073, -0.031, -0.001, -0.009,\n",
       "       -0.011, -0.02 , -0.006, -0.009, -0.027, -0.003,  0.45 ])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_[0].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e8661a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5\n",
    "# Let's find the least useful feature using the feature elimination technique.\n",
    "# Train a model using the same features and parameters as in Q4 (without rounding).\n",
    "# Now exclude each feature from this set and train a model without it. Record the accuracy for each model.\n",
    "# For each feature, calculate the difference between the original accuracy and the accuracy without the feature.\n",
    "    # Which of following feature has the smallest difference?\n",
    "    # 'industry'\n",
    "    # 'employment_status'\n",
    "    # 'lead_score'\n",
    "    # Note: The difference doesn't have to be positive.\n",
    "\n",
    "\n",
    "\n",
    "# Question 5: Feature Elimination (Finding the least useful feature)\n",
    "\n",
    "features_to_check = ['industry', 'employment_status', 'lead_score']\n",
    "accuracy_differences = {}\n",
    "\n",
    "# Use the one-hot encoded feature names for exclusion\n",
    "full_features = set(dv.get_feature_names_out())\n",
    "\n",
    "for feature_to_exclude in features_to_check:\n",
    "    \n",
    "    # Identify which columns to keep (all features excluding the current one)\n",
    "    # This logic is simplified; for OHE, this would need to exclude ALL columns \n",
    "    # generated by the categorical feature, but for the *generic* approach:\n",
    "    current_features_df = [f for f in all_features if f != feature_to_exclude]\n",
    "    \n",
    "    # Re-vectorize on the reduced feature set\n",
    "    dv_subset = DictVectorizer(sparse=False)\n",
    "    X_train_subset = dv_subset.fit_transform(df_train[current_features_df].to_dict(orient='records'))\n",
    "    X_val_subset = dv_subset.transform(df_val[current_features_df].to_dict(orient='records'))\n",
    "    \n",
    "    # Train a new model with the subset\n",
    "    model_subset = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "    model_subset.fit(X_train_subset, y_train)\n",
    "    \n",
    "    # Calculate new validation accuracy\n",
    "    y_pred_subset = model_subset.predict(X_val_subset)\n",
    "    accuracy_subset = (y_val == y_pred_subset).mean()\n",
    "    \n",
    "    # Calculate the difference (original accuracy - new accuracy)\n",
    "    diff = original_accuracy - accuracy_subset\n",
    "    accuracy_differences[feature_to_exclude] = diff\n",
    "\n",
    "# Find the feature with the smallest change (smallest absolute difference)\n",
    "# The \"least useful\" feature is the one whose removal causes the smallest absolute drop in performance.\n",
    "least_useful_feature = min(accuracy_differences, key=lambda k: abs(accuracy_differences[k]))\n",
    "\n",
    "print(f\"Accuracy differences: {accuracy_differences}\")\n",
    "print(f\"Feature with the smallest change (least useful): {least_useful_feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cda198",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Question 6\n",
    "# Now let's train a regularized logistic regression.\n",
    "# Let's try the following values of the parameter C: [0.01, 0.1, 1, 10, 100].\n",
    "# Train models using all the features as in Q4.\n",
    "# Calculate the accuracy on the validation dataset and round it to 3 decimal digits.\n",
    "# Which of these C leads to the best accuracy on the validation set?\n",
    "\n",
    "# 0.01\n",
    "# 0.1\n",
    "# 1\n",
    "# 10\n",
    "# 100\n",
    "# Note: If there are multiple options, select the smallest C."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
