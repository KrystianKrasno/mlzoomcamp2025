{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e727db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Homework - Week 4  - Evaluation \n",
    "# In this dataset our desired target for classification task will be `converted` variable - has the client signed up to the platform or not. \n",
    "\n",
    "# ### Data preparation\n",
    "# * Check if the missing values are presented in the features.\n",
    "# * If there are missing values:\n",
    "#     * For caterogiral features, replace them with 'NA'\n",
    "#     * For numerical features, replace with with 0.0 \n",
    "\n",
    "\n",
    "# Split the data into 3 parts: train/validation/test with 60%/20%/20% distribution. Use `train_test_split` function for that with `random_state=1`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0125ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Question 1: ROC AUC feature importance\n",
    "# ROC AUC could also be used to evaluate feature importance of numerical variables. \n",
    "# Let's do that\n",
    "# * For each numerical variable, use it as score (aka prediction) and compute the AUC with the `y` variable as ground truth.\n",
    "# * Use the training dataset for that\n",
    "\n",
    "# If your AUC is < 0.5, invert this variable by putting \"-\" in front\n",
    "\n",
    "# (e.g. `-df_train['balance']`)\n",
    "\n",
    "# AUC can go below 0.5 if the variable is negatively correlated with the target variable. You can change the direction of the correlation by negating this variable - then negative correlation becomes positive.\n",
    "\n",
    "# Which numerical variable (among the following 4) has the highest AUC?\n",
    "# - `lead_score`\n",
    "# - `number_of_courses_viewed`\n",
    "# - `interaction_count`\n",
    "# - `annual_income`\n",
    "\n",
    "# ### Question 2: Training the model\n",
    "\n",
    "# Apply one-hot-encoding using `DictVectorizer` and train the logistic regression with these parameters:\n",
    "\n",
    "# ```python\n",
    "# LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "# ```\n",
    "\n",
    "# What's the AUC of this model on the validation dataset? (round to 3 digits)\n",
    "# - 0.32\n",
    "# - 0.52\n",
    "# - 0.72\n",
    "# - 0.92\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040d6977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Question 3: Precision and Recall\n",
    "\n",
    "# Now let's compute precision and recall for our model.\n",
    "\n",
    "# * Evaluate the model on all thresholds from 0.0 to 1.0 with step 0.01\n",
    "# * For each threshold, compute precision and recall\n",
    "# * Plot them\n",
    "\n",
    "# At which threshold precision and recall curves intersect?\n",
    "\n",
    "# * 0.145\n",
    "# * 0.345\n",
    "# * 0.545\n",
    "# * 0.745"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2bb902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Question 4: F1 score\n",
    "\n",
    "# Precision and recall are conflicting - when one grows, the other goes down. That's why they are often combined into the F1 score - a metrics that takes into account both\n",
    "\n",
    "# This is the formula for computing F1:\n",
    "\n",
    "# $$F_1 = 2 \\cdot \\cfrac{P \\cdot R}{P + R}$$\n",
    "\n",
    "# Where $P$ is precision and $R$ is recall.\n",
    "\n",
    "# Let's compute F1 for all thresholds from 0.0 to 1.0 with increment 0.01\n",
    "\n",
    "# At which threshold F1 is maximal?\n",
    "\n",
    "# - 0.14\n",
    "# - 0.34\n",
    "# - 0.54\n",
    "# - 0.74\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b10c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Question 5: 5-Fold CV\n",
    "# Use the `KFold` class from Scikit-Learn to evaluate our model on 5 different folds:\n",
    "\n",
    "# ```\n",
    "# KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# ```\n",
    "\n",
    "# * Iterate over different folds of `df_full_train`\n",
    "# * Split the data into train and validation\n",
    "# * Train the model on train with these parameters: `LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)`\n",
    "# * Use AUC to evaluate the model on validation\n",
    "\n",
    "# How large is standard deviation of the scores across different folds?\n",
    "\n",
    "# - 0.0001\n",
    "# - 0.006\n",
    "# - 0.06\n",
    "# - 0.36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47d0440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Question 6: Hyperparameter Tuning\n",
    "\n",
    "# Now let's use 5-Fold cross-validation to find the best parameter `C`\n",
    "\n",
    "# * Iterate over the following `C` values: `[0.000001, 0.001, 1]`\n",
    "# * Initialize `KFold` with the same parameters as previously\n",
    "# * Use these parameters for the model: `LogisticRegression(solver='liblinear', C=C, max_iter=1000)`\n",
    "# * Compute the mean score as well as the std (round the mean and std to 3 decimal digits)\n",
    "\n",
    "# Which `C` leads to the best mean score?\n",
    "\n",
    "# - 0.000001\n",
    "# - 0.001\n",
    "# - 1\n",
    "\n",
    "# If you have ties, select the score with the lowest std. If you still have ties, select the smallest `C`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
